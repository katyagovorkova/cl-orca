{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workdir = \"/work/submit/deep1018/a3d3-hackathon-2023/\"\n",
    "workdir = \"/home/deep/github/a3d3/a3d3-workshop-2023/hep-ad-2023/\"\n",
    "background_dataset_name = workdir+'background_dataset.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(background_dataset_name) as f:\n",
    "    X_test = f['X_test'][()]\n",
    "    X_train = f['X_train'][()]\n",
    "    X_val = f['X_val'][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape to have MET, particles, jets in the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(X_test.reshape(-1, 19, 3)).to(dtype=torch.float32, device=device).transpose(1, 2)\n",
    "X_train = torch.from_numpy(X_train.reshape(-1, 19, 3)).to(dtype=torch.float32, device=device).transpose(1, 2)\n",
    "X_val = torch.from_numpy(X_val.reshape(-1, 19, 3)).to(dtype=torch.float32, device=device).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num dimensions {X_train.shape[-1]}; Num channels {X_train.shape[-2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEPDataSet(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HEPDataSet(X_train)\n",
    "val_set = HEPDataSet(X_val)\n",
    "test_set = HEPDataSet(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4000\n",
    "test_batch_size = 10000\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_set, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    val_set, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_set, batch_size=test_batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num batches for training = {len(train_data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(train_data_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define similarity loss\n",
    "\n",
    "All of SM particles embedded to a space where we enforce similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified implemented from https://github.com/violatingcp/codec/blob/main/losses.py\n",
    "class VICRegLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "        x_mu = x.mean(dim=0)\n",
    "        x_std = x.std(dim=0) + 1e-2\n",
    "        y_mu = y.mean(dim=0)\n",
    "        y_std = y.std(dim=0) + 1e-2\n",
    "\n",
    "        x = (x - x_mu)/x_std\n",
    "        y = (y - y_mu)/y_std\n",
    "\n",
    "        N = x.size(0)\n",
    "        D = x.size(-1)\n",
    "\n",
    "        std_loss = torch.mean(F.relu(1 - x_std, inplace=False)) / 2\n",
    "        std_loss += torch.mean(F.relu(1 - y_std, inplace=False)) / 2\n",
    "\n",
    "        cov_x = (x.transpose(1, 2).contiguous() @ x) / (N - 1)\n",
    "        cov_y = (y.transpose(1, 2).contiguous() @ y) / (N - 1)\n",
    "\n",
    "        cov_loss = self.off_diagonal(cov_x).pow_(2).sum().div(D)\n",
    "        cov_loss += self.off_diagonal(cov_y).pow_(2).sum().div(D)\n",
    "\n",
    "        return repr_loss + cov_loss + std_loss\n",
    "\n",
    "    def off_diagonal(self, x):\n",
    "        num_batch, n, m = x.shape\n",
    "        assert n == m\n",
    "        # All off diagonal elements from complete batch flattened\n",
    "        return x.flatten(start_dim=1)[...,:-1].view(num_batch, n - 1, n + 1)[...,1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicreg_loss = VICRegLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.nn.nets import ResidualNet\n",
    "from nflows import transforms, distributions, flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityEmbedding(nn.Module):\n",
    "    \"\"\"An embedding with resnets\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = ResidualNet(19, 1, 35)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_embedding = self.resnet(x)\n",
    "        return res_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_embedding = SimilarityEmbedding().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(similarity_embedding.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_1 = torch.optim.lr_scheduler.ConstantLR(optimizer, total_iters=5)\n",
    "scheduler_2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "scheduler_3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer, schedulers=[scheduler_1, scheduler_2, scheduler_3], milestones=[5, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_embedding(val).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_sim_loss = 0.\n",
    "    last_sim_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(train_data_loader, 1):\n",
    "        # only applicable to the final batch\n",
    "        if val.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        # embed entire batch with first value of the batch repeated\n",
    "        first_val_repeated = val[0].repeat(batch_size, 1, 1)\n",
    "\n",
    "        embedded_values_aug = similarity_embedding(first_val_repeated)\n",
    "        embedded_values_orig = similarity_embedding(val)\n",
    "\n",
    "        similar_embedding_loss = vicreg_loss(embedded_values_aug, embedded_values_orig)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        similar_embedding_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Gather data and report\n",
    "        running_sim_loss += similar_embedding_loss.item()\n",
    "        if idx % 500 == 0:\n",
    "            last_sim_loss = running_sim_loss / 500\n",
    "            print(' Avg. train loss/batch after {} batches = {:.4f}'.format(idx, last_sim_loss))\n",
    "            tb_x = epoch_index * len(train_data_loader) + idx\n",
    "            tb_writer.add_scalar('SimLoss/train', last_sim_loss, tb_x)\n",
    "            running_sim_loss = 0.\n",
    "    return last_sim_loss\n",
    "\n",
    "\n",
    "def val_one_epoch(epoch_index, tb_writer):\n",
    "    running_sim_loss = 0.\n",
    "    last_sim_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(val_data_loader, 1):\n",
    "        if val.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        first_val_repeated = val[0].repeat(batch_size, 1, 1)\n",
    "\n",
    "        embedded_values_aug = similarity_embedding(first_val_repeated)\n",
    "        embedded_values_orig = similarity_embedding(val)\n",
    "\n",
    "        similar_embedding_loss = vicreg_loss(embedded_values_aug, embedded_values_orig)\n",
    "\n",
    "        running_sim_loss += similar_embedding_loss.item()\n",
    "        if idx % 50 == 0:\n",
    "            last_sim_loss = running_sim_loss / 50\n",
    "            tb_x = epoch_index * len(val_data_loader) + idx + 1\n",
    "            tb_writer.add_scalar('SimLoss/val', last_sim_loss, tb_x)\n",
    "            tb_writer.flush()\n",
    "            running_sim_loss = 0.\n",
    "    tb_writer.flush()\n",
    "    return last_sim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(\"hep_sim_together_round_2\", comment=\"Similarity with LR=1e-3\", flush_secs=5)\n",
    "# epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # UNCOMMENT AND RUN TO TRAIN FROM SCRATCH\n",
    "# EPOCHS = 10\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     print('EPOCH {}:'.format(epoch_number + 1))\n",
    "#     # Gradient tracking\n",
    "#     similarity_embedding.train(True)\n",
    "#     avg_train_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "#     # no gradient tracking, for validation\n",
    "#     similarity_embedding.train(False)\n",
    "#     avg_val_loss = val_one_epoch(epoch_number, writer)\n",
    "    \n",
    "#     print(f\"Train/Val Sim Loss after epoch: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "\n",
    "#     epoch_number += 1\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = workdir+\"similarity_model_together.pt\"\n",
    "# torch.save(similarity_embedding.state_dict(), PATH)\n",
    "similarity_embedding.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot embedded values for testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vals = []\n",
    "\n",
    "for idx, val in enumerate(test_data_loader, 1):\n",
    "    if val.shape[0] != test_batch_size:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        sim_val = similarity_embedding(val)\n",
    "        sim_vals.append(sim_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vals = torch.stack(sim_vals).reshape(len(sim_vals)*test_batch_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "figure = corner.corner(\n",
    "    sim_vals.cpu().numpy(), color=\"C1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to count trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num trainable parameters in embedding net: {count_parameters(similarity_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze weights for embedding net\n",
    "for name, param in similarity_embedding.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create small Normalizing flow to transform above latent space\n",
    "\n",
    "This is used as a density estimation on the similar subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "base_dist = distributions.StandardNormal(shape=[3])\n",
    "\n",
    "transform_layers = []\n",
    "for _ in range(num_layers):\n",
    "    transform_layers.append(transforms.permutations.ReversePermutation(features=3))\n",
    "    transform_layers.append(\n",
    "        transforms.autoregressive.MaskedAffineAutoregressiveTransform(\n",
    "            features=3,\n",
    "            hidden_features=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "transform_obj = transforms.CompositeTransform(transform_layers)\n",
    "\n",
    "flow_obj = flows.Flow(transform_obj, base_dist).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(flow_obj.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num training parameters in flow = {count_parameters(flow_obj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT AND RUN TO TRAIN FROM SCRATCH\n",
    "# num_iter = 10\n",
    "# for i in range(num_iter):\n",
    "#     # use the val_data_loader from previous step; this is only for quicker training\n",
    "#     # alternatively restrict to fewer batches from train data loader\n",
    "#     for idx, val in enumerate(val_data_loader):\n",
    "#         # to work with the reshaping below\n",
    "#         if val.shape[0] != batch_size:\n",
    "#             continue\n",
    "\n",
    "#         sim_val = similarity_embedding(val).reshape(4000, 3)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = -flow_obj.log_prob(inputs=sim_val).mean()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"Avg. log prob = {-loss.item():.3f}\")\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_WEIGHTS = workdir+\"maf_flow.pt\"\n",
    "# torch.save(flow_obj.state_dict(), FLOW_WEIGHTS)\n",
    "flow_obj.load_state_dict(torch.load(FLOW_WEIGHTS, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Foreground (BSM) vs Background (SM) data\n",
    "\n",
    "Check and compare p-values for the SM and BSM particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SM testing set\n",
    "# pick individual events from the test set and get p-val, stop at 50K to be fast\n",
    "sm_p_vals = []\n",
    "for idx, val in enumerate(test_data_loader, 1):\n",
    "    if val.shape[0] != test_batch_size:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        log_p = flow_obj.log_prob(similarity_embedding(val).reshape(test_batch_size, 3))\n",
    "        sm_p_vals.append(log_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSM dataset\n",
    "\n",
    "Same for the BSM dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $A \\rightarrow 4l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(workdir+'signals_dataset.h5') as f:\n",
    "    a_4l = f['Data'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_4l = HEPDataSet(torch.from_numpy(a_4l.reshape(-1, 19, 3)).to(dtype=torch.float32, device=device).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_4l_loader = DataLoader(a_4l, shuffle=True, batch_size=test_batch_size)\n",
    "len(a_4l_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_4l_p_vals = []\n",
    "for idx, val in enumerate(a_4l_loader, 1):\n",
    "    if val.shape[0] != test_batch_size:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        log_p = flow_obj.log_prob(similarity_embedding(val).reshape(test_batch_size, 3))\n",
    "        a_4l_p_vals.append(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_p_vals = torch.stack(sm_p_vals).flatten()\n",
    "a_4l_p_vals = torch.stack(a_4l_p_vals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A to 4L p-val median {torch.median(a_4l_p_vals):.3f}\")\n",
    "print(f\"A to 4L p-val mean {torch.mean(a_4l_p_vals):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing SM p-vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SM p-val median {torch.median(sm_p_vals):.3f}\")\n",
    "print(f\"SM p-val mean {torch.mean(sm_p_vals):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $h \\rightarrow \\tau\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(workdir+'signals_hToTauTau_dataset.h5') as f:\n",
    "    h_tau_tau = f['Data'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_tau = HEPDataSet(torch.from_numpy(h_tau_tau.reshape(-1, 19, 3)).to(dtype=torch.float32, device=device).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_tau_loader = DataLoader(h_tau_tau, shuffle=True, batch_size=test_batch_size)\n",
    "len(h_tau_tau_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_tau_p_vals = []\n",
    "for idx, val in enumerate(h_tau_tau_loader, 1):\n",
    "    if val.shape[0] != test_batch_size:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        log_p = flow_obj.log_prob(similarity_embedding(val).reshape(test_batch_size, 3))\n",
    "        h_tau_tau_p_vals.append(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_tau_p_vals = torch.stack(h_tau_tau_p_vals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"h to tau-tau p-val median {torch.median(h_tau_tau_p_vals):.3f}\")\n",
    "print(f\"h to tau-tau p-val mean {torch.mean(h_tau_tau_p_vals):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $h^{\\pm} \\rightarrow \\tau\\nu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(workdir+'signals_hChToTauTau_dataset.h5') as f:\n",
    "    h_tau_nu = f['Data'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_nu = HEPDataSet(torch.from_numpy(h_tau_nu.reshape(-1, 19, 3)).to(device=device, dtype=torch.float32).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_nu_loader = DataLoader(h_tau_nu, shuffle=True, batch_size=test_batch_size)\n",
    "len(h_tau_nu_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_nu_p_vals = []\n",
    "for idx, val in enumerate(h_tau_nu_loader, 1):\n",
    "    if val.shape[0] != test_batch_size:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        log_p = flow_obj.log_prob(similarity_embedding(val).reshape(test_batch_size, 3))\n",
    "        h_tau_nu_p_vals.append(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tau_nu_p_vals = torch.stack(h_tau_nu_p_vals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"h to tau-nu p-val median {torch.median(h_tau_nu_p_vals):.3f}\")\n",
    "print(f\"h to tau-nu p-val mean {torch.mean(h_tau_nu_p_vals):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $LQ \\rightarrow b\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(workdir+\"signals_leptoquark_dataset.h5\") as f:\n",
    "    leptoquark = f['Data'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leptoquark = HEPDataSet(torch.from_numpy(leptoquark.reshape(-1, 19, 3)).to(dtype=torch.float32, device=device).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leptoquark_loader = DataLoader(leptoquark, shuffle=True, batch_size=test_batch_size)\n",
    "len(leptoquark_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leptoquark_p_vals = []\n",
    "for idx, val in enumerate(leptoquark_loader, 1):\n",
    "    if val.shape[0] != test_batch_size:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        log_p = flow_obj.log_prob(similarity_embedding(val).reshape(test_batch_size, 3))\n",
    "        leptoquark_p_vals.append(log_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leptoquark_p_vals = torch.stack(leptoquark_p_vals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LQ to b-tau p-val median {torch.median(leptoquark_p_vals):.3f}\")\n",
    "print(f\"LQ to b-tau p-val mean {torch.mean(leptoquark_p_vals):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency vs. False alarm of all BSM events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricting scale of the plot\n",
    "\n",
    "Here, the plot is zoomed to higher p-values. Most of the BSM distribution has significantly negative p-vals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sm_p_vals.cpu(), range=(-10, 6), bins=50, label='SM',\n",
    "         density=True, alpha=0.8)\n",
    "plt.hist(leptoquark_p_vals.cpu(), range=(-10, 6), bins=50, label='$LQ \\\\rightarrow b\\\\tau$',\n",
    "         density=True, alpha=0.8, histtype='step', linewidth=2)\n",
    "plt.hist(h_tau_tau_p_vals.cpu(), range=(-10, 6), bins=50, label='$h \\\\rightarrow \\\\tau\\\\tau$',\n",
    "         density=True, alpha=0.8, histtype='step', linewidth=2)\n",
    "plt.hist(h_tau_nu_p_vals.cpu(), range=(-10, 6), bins=50, label='$h^{\\pm} \\\\rightarrow \\\\tau\\\\nu$',\n",
    "         density=True, alpha=0.8, histtype='step', linewidth=2)\n",
    "plt.hist(a_4l_p_vals.cpu(), range=(-10, 6), bins=50, label='$A \\\\rightarrow 4l$',\n",
    "         density=True, alpha=0.8, histtype='step', linewidth=2)\n",
    "plt.title(\"SM and BSM event p-vals\")\n",
    "plt.axvline(-5, label='Ex. Threshold', color='red')\n",
    "plt.annotate(\"\", xy=(-5.1, 0.5), xytext=(-7.5, 0.5),\n",
    "             arrowprops=dict(arrowstyle=\"<-\"))\n",
    "plt.text(-7.5, 0.52, \"Detection\")\n",
    "plt.xlabel(\"log prob.\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import numpy as np\n",
    "\n",
    "colors = cycle(['red', 'green', 'blue', 'black'])\n",
    "linestyles = cycle(['--', '-.', ':', '-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(-50, 6, 500)\n",
    "\n",
    "p_vals = {\n",
    "    'h_tau_tau': h_tau_tau_p_vals.cpu().numpy(),\n",
    "    'h_tau_nu': h_tau_nu_p_vals.cpu().numpy(),\n",
    "    'A_4l': a_4l_p_vals.cpu().numpy(),\n",
    "    'leptoquark': leptoquark_p_vals.cpu().numpy()\n",
    "}\n",
    "\n",
    "efficiencies = dict.fromkeys(p_vals)\n",
    "false_alarms = dict.fromkeys(p_vals)\n",
    "\n",
    "\n",
    "for k, p_val in p_vals.items():\n",
    "    eff = []\n",
    "    false = []\n",
    "    for threshold in thresholds:\n",
    "        _eff = np.sum(p_val < threshold)\n",
    "        _false = np.sum(sm_p_vals.cpu().numpy() < threshold)\n",
    "        eff.append(_eff)\n",
    "        false.append(_false)\n",
    "    efficiencies[k] = np.array(eff)/len(p_val)\n",
    "    false_alarms[k] = np.array(false)/len(sm_p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for k in efficiencies:\n",
    "    plt.loglog(false_alarms[k], efficiencies[k], label=k,\n",
    "               color=next(colors), linestyle=next(linestyles), alpha=0.6)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim((1e-4, 1))\n",
    "plt.ylabel(\"Efficiency\")\n",
    "plt.xlabel(\"False Alarm\")\n",
    "plt.axvline(x=1e-5, c='r')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
